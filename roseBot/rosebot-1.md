#语音助手是这样子的（一）

近年来，随着人工智能技术的逐步落地，很多智能化的产品进入了人们的日常生活。2011年苹果公司在iPhone4S手机上推出了一款里程碑式的智能产品“Siri”，用户只需要通过对手机说一些指令，手机就可以自动完成很多操作。我们最常用的场景莫过于对着Siri说“帮我设置明天早上六点的闹钟”，而Siri会理解我们的指令，把手机上的闹钟设置好，并且回答我们“好的，已经把闹钟设置到了明天早上六点”。我们是否想过这样一个智能产品是如何设计与开发出来的呢？本系列博客就将带你揭秘智能语音助手背后的技术。


##从场景开始

智能语音助手看似智能，其实是很傻瓜的，因为它只能应对设计好的有限场景，所以要让它能够完成用户日常使用智能手机的常用操作就需要设计一个又一个的场景，我们来看一个比较简单的场景：
> 用户：设置闹钟<br/>
> Siri：请问您需要设置几点的闹钟？<br/>
> 用户：明天早上六点的<br/>
> Siri：好的，已经把闹钟设置到了明天早上六点<br/>

在这个场景下，Siri是如何听懂用户的指令的呢？又是如何识别出用户的指令就是设置闹钟这样一个意图呢？就算它知道了设置闹钟，又是怎么就知道向用户确定设置的时间的呢？用户提出的具体时间Siri又是怎样理解的呢？


##背后的技术

语音助手背后的技术由三大块组成：语音识别（Auto Speech Recognition）、语义理解（Semantic Understanding）、语音合成（Text To Speech）
![Core Technology](https://raw.githubusercontent.com/rouseway/blogs/master/roseBot/rosebot-3.jpg)

##语义理解是核心
如果我们不考虑与用户的交互都是语音效果，全当是文字输入的互动，那么前文提到的场景可以拆解为几个核心的技术点：<br/>

- **自然语言理解（Natrual Language Understanding）：** 用户的指令是语言，如何理解用户的意图，并且从用户的输入中提取出相关的信息，比如：识别设置闹钟的需求，提取出闹钟需要设置在“明天早上六点”的关键信息
- **对话管理（Dialogue Management）：**语音助手应该按照怎样的逻辑与用户进行交互，听到一个指令后应该执行怎样的动作或响应
- **自然语言生成（Natrual Language Generation）：**针对用户的指令，语音助手


##一步步实现
接下来，我们将一步一步地去实现一个简单的语音助手（不包含语音部分），我们将在本系列博客的后续部分逐步展开每一步的设计与实现，在语音助手中的NLG并没有太多丰富的表达形式，所以我们将弱化这部分的技术，将其同对话管理合并。对话管理我们将使用开源的工具OpenDial来实现，而NLU我们将采用普遍使用模板匹配的方法，所以后续文章将组织为：<br/>
1. 用OpenDial实现对话设计<br/>
2. 模板匹配方式的NLU<br/>
3. 更高级的方法与技术<br/>